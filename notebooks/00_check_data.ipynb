{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bf83094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_v2_sharded.py\n",
    "from __future__ import annotations\n",
    "import os, json, glob\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "\n",
    "def _load_json(path: str) -> Dict:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def _first_existing(paths: List[str]) -> Optional[str]:\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def _np_dtype(dtype_str: str):\n",
    "    # metadataに \"float32\" / \"int32\" / \"uint16\" などが来る想定\n",
    "    return getattr(np, dtype_str) if hasattr(np, dtype_str) else np.dtype(dtype_str)\n",
    "\n",
    "# -------------------------\n",
    "# Dataset\n",
    "# -------------------------\n",
    "\n",
    "@dataclass\n",
    "class V2Spec:\n",
    "    token_grid: Tuple[int, int, int] = (6, 32, 32)  # (T_total, H, W) for one sample\n",
    "    state_grid: Tuple[int, int] = (64, 25)          # (T_state, state_dim)\n",
    "    token_dtype: np.dtype = np.int32\n",
    "    state_dtype: np.dtype = np.float32\n",
    "    segment_dtype: np.dtype = np.int64              # segment_idx dtype (often int64)\n",
    "    # If segment_idx stores [start,end) offsets into flattened token stream\n",
    "    segment_is_pairs: bool = True\n",
    "\n",
    "class ShardedCompressionV2Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Train/Val v2.0 Sharded format.\n",
    "\n",
    "    Expected layout (root/split):\n",
    "      video_{shard}.bin\n",
    "      segment_idx_{shard}.bin\n",
    "      states_{shard}.bin\n",
    "      metadata.json or metadata_{shard}.json (optional but recommended)\n",
    "\n",
    "    Returns per sample:\n",
    "      z: LongTensor (6, 32, 32)  tokens\n",
    "      s: FloatTensor (64, 25)    states\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: str,\n",
    "        split: str = \"train\",  # \"train\" / \"val\" / \"test_v2.0\" etc.\n",
    "        spec: Optional[V2Spec] = None,\n",
    "        use_memmap: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.root = os.path.join(root_dir, split)\n",
    "        self.spec = spec or V2Spec()\n",
    "        self.use_memmap = use_memmap\n",
    "\n",
    "        # ---- metadata (overall + per-shard) ----\n",
    "        meta_overall_path = _first_existing([\n",
    "            os.path.join(self.root, \"metadata.json\"),\n",
    "            os.path.join(self.root, \"metadata_overall.json\"),\n",
    "        ])\n",
    "        self.meta_overall = _load_json(meta_overall_path) if meta_overall_path else None\n",
    "\n",
    "        # shard files\n",
    "        self.video_files = sorted(glob.glob(os.path.join(self.root, \"segment_indices/videos/video_*.bin\")))\n",
    "        self.seg_files   = sorted(glob.glob(os.path.join(self.root, \"segment_indices/segment_idx_*.bin\")))\n",
    "        self.state_files = sorted(glob.glob(os.path.join(self.root, \"robot_states/states_*.bin\")))\n",
    "\n",
    "        if len(self.video_files) == 0:\n",
    "            raise FileNotFoundError(f\"No video_*.bin under {self.root}\")\n",
    "\n",
    "        # test_v2.0 may omit segment_idx_*.bin (as your description says \"similar structure\")\n",
    "        self.has_segments = len(self.seg_files) == len(self.video_files) and len(self.seg_files) > 0\n",
    "        if self.has_segments:\n",
    "            assert len(self.state_files) == len(self.video_files), \"states shard count mismatch\"\n",
    "        else:\n",
    "            # If no segment_idx, we assume each shard already corresponds to one sample or fixed-length packing.\n",
    "            # We'll support a fixed-length packing fallback.\n",
    "            assert len(self.state_files) == len(self.video_files), \"states shard count mismatch\"\n",
    "\n",
    "        # ---- Build global index: list of (shard_id, local_sample_id) ----\n",
    "        self.shard_sample_counts: List[int] = []\n",
    "        self.segment_arrays: List[np.ndarray] = []\n",
    "        self.video_mmaps: List[np.memmap | np.ndarray] = []\n",
    "        self.state_mmaps: List[np.memmap | np.ndarray] = []\n",
    "\n",
    "        T, H, W = self.spec.token_grid\n",
    "        tok_per_sample = T * H * W\n",
    "        St, Sd = self.spec.state_grid\n",
    "        state_per_sample = St * Sd\n",
    "\n",
    "        for sid in range(len(self.video_files)):\n",
    "            # per-shard metadata (optional)\n",
    "            meta_shard_path = _first_existing([\n",
    "                os.path.join(self.root, f\"metadata_{sid}.json\"),\n",
    "                os.path.join(self.root, f\"metadata_{os.path.basename(self.video_files[sid]).split('_')[1].split('.')[0]}.json\"),\n",
    "            ])\n",
    "            meta_shard = _load_json(meta_shard_path) if meta_shard_path else None\n",
    "\n",
    "            # dtype overrides from metadata if present\n",
    "            token_dtype = self.spec.token_dtype\n",
    "            state_dtype = self.spec.state_dtype\n",
    "            seg_dtype   = self.spec.segment_dtype\n",
    "\n",
    "            if meta_shard:\n",
    "                # Best-effort keys (dataset提供側でキー名が違う可能性があるため)\n",
    "                if \"video_dtype\" in meta_shard:\n",
    "                    token_dtype = _np_dtype(meta_shard[\"video_dtype\"])\n",
    "                if \"states_dtype\" in meta_shard:\n",
    "                    state_dtype = _np_dtype(meta_shard[\"states_dtype\"])\n",
    "                if \"segment_idx_dtype\" in meta_shard:\n",
    "                    seg_dtype = _np_dtype(meta_shard[\"segment_idx_dtype\"])\n",
    "\n",
    "            # memmap open\n",
    "            vpath = self.video_files[sid]\n",
    "            spath = self.state_files[sid]\n",
    "            video = np.memmap(vpath, mode=\"r\", dtype=token_dtype) if use_memmap else np.fromfile(vpath, dtype=token_dtype)\n",
    "            states = np.memmap(spath, mode=\"r\", dtype=state_dtype) if use_memmap else np.fromfile(spath, dtype=state_dtype)\n",
    "\n",
    "            self.video_mmaps.append(video)\n",
    "            self.state_mmaps.append(states)\n",
    "\n",
    "            if self.has_segments:\n",
    "                segpath = self.seg_files[sid]\n",
    "                seg = np.memmap(segpath, mode=\"r\", dtype=seg_dtype) if use_memmap else np.fromfile(segpath, dtype=seg_dtype)\n",
    "                seg = np.asarray(seg)  # safe view\n",
    "                # We expect pairs (start,end) -> shape (N,2)\n",
    "                if seg.ndim == 1:\n",
    "                    if seg.size % 2 == 0:\n",
    "                        seg = seg.reshape(-1, 2)\n",
    "                    else:\n",
    "                        # could be boundary list [b0,b1,...,bN], interpret as start boundaries\n",
    "                        # We'll convert to pairs\n",
    "                        boundaries = seg\n",
    "                        starts = boundaries[:-1]\n",
    "                        ends = boundaries[1:]\n",
    "                        seg = np.stack([starts, ends], axis=1)\n",
    "                self.segment_arrays.append(seg)\n",
    "                n = seg.shape[0]\n",
    "            else:\n",
    "                # fallback: infer sample count by fixed packing\n",
    "                n = int(video.size // tok_per_sample)\n",
    "                # optional sanity check on states\n",
    "                n_states = int(states.size // state_per_sample)\n",
    "                n = min(n, n_states)\n",
    "\n",
    "            self.shard_sample_counts.append(n)\n",
    "\n",
    "        # global prefix sums for O(logN) indexing\n",
    "        self._prefix = np.cumsum([0] + self.shard_sample_counts).tolist()\n",
    "\n",
    "        # cache for shapes\n",
    "        self._tok_per_sample = tok_per_sample\n",
    "        self._state_per_sample = state_per_sample\n",
    "        self._T, self._H, self._W = self.spec.token_grid\n",
    "        self._St, self._Sd = self.spec.state_grid\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._prefix[-1]\n",
    "\n",
    "    def _locate(self, idx: int) -> Tuple[int, int]:\n",
    "        # binary search over prefix sums\n",
    "        # find sid such that prefix[sid] <= idx < prefix[sid+1]\n",
    "        lo, hi = 0, len(self.shard_sample_counts)\n",
    "        while lo + 1 < hi:\n",
    "            mid = (lo + hi) // 2\n",
    "            if self._prefix[mid] <= idx:\n",
    "                lo = mid\n",
    "            else:\n",
    "                hi = mid\n",
    "        sid = lo\n",
    "        local = idx - self._prefix[sid]\n",
    "        return sid, local\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        sid, local = self._locate(idx)\n",
    "\n",
    "        video = self.video_mmaps[sid]\n",
    "        states = self.state_mmaps[sid]\n",
    "\n",
    "        # ---- tokens ----\n",
    "        if self.has_segments:\n",
    "            seg = self.segment_arrays[sid]\n",
    "            start, end = int(seg[local, 0]), int(seg[local, 1])\n",
    "            flat = video[start:end]\n",
    "            # Many datasets store exactly one sample worth of tokens per segment\n",
    "            # Ensure length matches expected (tok_per_sample)\n",
    "            if flat.size != self._tok_per_sample:\n",
    "                # If segment spans more, you may need to crop/choose window.\n",
    "                # For Revontuli compression: expect exactly 6*32*32 tokens.\n",
    "                flat = flat[: self._tok_per_sample]\n",
    "            z = flat.reshape(self._T, self._H, self._W)\n",
    "        else:\n",
    "            # fixed packing fallback\n",
    "            off = local * self._tok_per_sample\n",
    "            z = np.asarray(video[off: off + self._tok_per_sample]).reshape(self._T, self._H, self._W)\n",
    "\n",
    "        z = torch.from_numpy(np.asarray(z)).long()\n",
    "\n",
    "        # ---- states ----\n",
    "        off_s = local * self._state_per_sample\n",
    "        s = np.asarray(states[off_s: off_s + self._state_per_sample]).reshape(self._St, self._Sd)\n",
    "        s = torch.from_numpy(s).float()\n",
    "\n",
    "        return z, s\n",
    "\n",
    "\n",
    "# --------------- Optional: State index definition for clarity ---------------\n",
    "STATE_INDEX = {\n",
    "    0: \"HIP_YAW\",\n",
    "    1: \"HIP_ROLL\",\n",
    "    2: \"HIP_PITCH\",\n",
    "    3: \"KNEE_PITCH\",\n",
    "    4: \"ANKLE_ROLL\",\n",
    "    5: \"ANKLE_PITCH\",\n",
    "    6: \"LEFT_SHOULDER_PITCH\",\n",
    "    7: \"LEFT_SHOULDER_ROLL\",\n",
    "    8: \"LEFT_SHOULDER_YAW\",\n",
    "    9: \"LEFT_ELBOW_PITCH\",\n",
    "    10: \"LEFT_ELBOW_YAW\",\n",
    "    11: \"LEFT_WRIST_PITCH\",\n",
    "    12: \"LEFT_WRIST_ROLL\",\n",
    "    13: \"RIGHT_SHOULDER_PITCH\",\n",
    "    14: \"RIGHT_SHOULDER_ROLL\",\n",
    "    15: \"RIGHT_SHOULDER_YAW\",\n",
    "    16: \"RIGHT_ELBOW_PITCH\",\n",
    "    17: \"RIGHT_ELBOW_YAW\",\n",
    "    18: \"RIGHT_WRIST_PITCH\",\n",
    "    19: \"RIGHT_WRIST_ROLL\",\n",
    "    20: \"NECK_PITCH\",\n",
    "    21: \"LEFT_HAND_CLOSURE\",\n",
    "    22: \"RIGHT_HAND_CLOSURE\",\n",
    "    23: \"LINEAR_VELOCITY\",\n",
    "    24: \"ANGULAR_VELOCITY\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4add6c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = V2Spec(\n",
    "    token_grid=(8, 8, 8),\n",
    "    state_grid=(64, 25),\n",
    "    token_dtype=np.uint32,\n",
    "    state_dtype=np.float32,\n",
    "    segment_dtype=np.int16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce4bebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ShardedCompressionV2Dataset(\n",
    "    root_dir=\"/root/work/data/raw/\",\n",
    "    split=\"train_v2.0\",          # train / val / test_v2.0\n",
    "    spec=spec,\n",
    "    use_memmap=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "045e648a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seg bytes: 450168\n",
      "vid bytes: 81358848\n",
      "seg head bytes: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "split_dir = \"/root/work/data/raw/train_v2.0\"  # あなたの split フォルダに合わせて\n",
    "seg_path  = os.path.join(split_dir, \"segment_indices/segment_idx_0.bin\")\n",
    "vid_path  = os.path.join(split_dir, \"segment_indices/videos/video_0.bin\")\n",
    "\n",
    "print(\"seg bytes:\", os.path.getsize(seg_path))\n",
    "print(\"vid bytes:\", os.path.getsize(vid_path))\n",
    "\n",
    "# 先頭 64 byte を生で見る（形式推定に最強）\n",
    "head = np.fromfile(seg_path, dtype=np.uint8, count=64)\n",
    "print(\"seg head bytes:\", head.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b4d19ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferred seg dtype: <class 'numpy.int16'> offset: 0 n_elems: 225084\n",
      "seg first 20 elems: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "pairs[0:5]:\n",
      " [[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "lens[0:5]: [0, 0, 0, 0, 0]\n",
      "video n_elems: 20339712\n",
      "max end (first 100): 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, os\n",
    "\n",
    "def infer_memmap_dtype_and_offset(path: str):\n",
    "    size = os.path.getsize(path)\n",
    "    dtype_candidates = [np.int32, np.uint32, np.int64, np.uint64, np.int16, np.uint16]\n",
    "    offset_candidates = [0, 4, 8, 16, 32]\n",
    "    feasible = []\n",
    "    for off in offset_candidates:\n",
    "        if size <= off:\n",
    "            continue\n",
    "        rem = size - off\n",
    "        for dt in dtype_candidates:\n",
    "            if rem % np.dtype(dt).itemsize == 0:\n",
    "                feasible.append((dt, off, rem // np.dtype(dt).itemsize))\n",
    "    if not feasible:\n",
    "        raise ValueError(f\"Cannot infer dtype/offset: {path}, size={size}\")\n",
    "    feasible_sorted = sorted(feasible, key=lambda x: (x[2] % 2 != 0, np.dtype(x[0]).itemsize))\n",
    "    return feasible_sorted[0]\n",
    "\n",
    "dt, off, n = infer_memmap_dtype_and_offset(seg_path)\n",
    "print(\"inferred seg dtype:\", dt, \"offset:\", off, \"n_elems:\", n)\n",
    "\n",
    "seg = np.memmap(seg_path, mode=\"r\", dtype=dt, offset=off)\n",
    "seg = np.asarray(seg)\n",
    "\n",
    "print(\"seg first 20 elems:\", seg[:20].tolist())\n",
    "\n",
    "# pairs / boundary の両方を試して見る\n",
    "if seg.size % 2 == 0:\n",
    "    pairs = seg.reshape(-1, 2)\n",
    "    print(\"pairs[0:5]:\\n\", pairs[:5])\n",
    "    lens = pairs[:5,1] - pairs[:5,0]\n",
    "    print(\"lens[0:5]:\", lens.tolist())\n",
    "else:\n",
    "    b = seg\n",
    "    pairs = np.stack([b[:-1], b[1:]], axis=1)\n",
    "    print(\"boundary->pairs[0:5]:\\n\", pairs[:5])\n",
    "    lens = pairs[:5,1] - pairs[:5,0]\n",
    "    print(\"lens[0:5]:\", lens.tolist())\n",
    "\n",
    "# video 側の要素数（dtypeは今のspecに合わせて）\n",
    "video = np.memmap(vid_path, mode=\"r\", dtype=np.int32)  # token_dtype が int32 の想定\n",
    "print(\"video n_elems:\", video.size)\n",
    "print(\"max end (first 100):\", int(pairs[:100,1].max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57158fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seg bytes: 450168\n",
      "\n",
      "--- dtype=<class 'numpy.int32'> ---\n",
      "n_elems: 112542\n",
      "nonzero count: 111736\n",
      "first nonzero index: 806 value: 1\n",
      "head 20: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "pairs[0:5]:\n",
      " [[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "lens[0:5]: [0, 0, 0, 0, 0]\n",
      "\n",
      "--- dtype=<class 'numpy.uint32'> ---\n",
      "n_elems: 112542\n",
      "nonzero count: 111736\n",
      "first nonzero index: 806 value: 1\n",
      "head 20: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "pairs[0:5]:\n",
      " [[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "lens[0:5]: [0, 0, 0, 0, 0]\n",
      "\n",
      "--- dtype=<class 'numpy.int16'> ---\n",
      "n_elems: 225084\n",
      "nonzero count: 111736\n",
      "first nonzero index: 1612 value: 1\n",
      "head 20: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "pairs[0:5]:\n",
      " [[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "lens[0:5]: [0, 0, 0, 0, 0]\n",
      "\n",
      "--- dtype=<class 'numpy.uint16'> ---\n",
      "n_elems: 225084\n",
      "nonzero count: 111736\n",
      "first nonzero index: 1612 value: 1\n",
      "head 20: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "pairs[0:5]:\n",
      " [[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "lens[0:5]: [0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np\n",
    "\n",
    "seg_path = \"/root/work/data/raw/train_v2.0/segment_indices/segment_idx_0.bin\"\n",
    "\n",
    "print(\"seg bytes:\", os.path.getsize(seg_path))\n",
    "\n",
    "for dt in [np.int32, np.uint32, np.int16, np.uint16]:\n",
    "    arr = np.memmap(seg_path, mode=\"r\", dtype=dt)\n",
    "    arr = np.asarray(arr)\n",
    "    nz = np.flatnonzero(arr)\n",
    "    print(f\"\\n--- dtype={dt} ---\")\n",
    "    print(\"n_elems:\", arr.size)\n",
    "    print(\"nonzero count:\", nz.size)\n",
    "    if nz.size > 0:\n",
    "        i = int(nz[0])\n",
    "        print(\"first nonzero index:\", i, \"value:\", int(arr[i]))\n",
    "        print(\"head 20:\", arr[:20].tolist())\n",
    "        # pairs解釈（start,end）として最初の5つ\n",
    "        if arr.size % 2 == 0:\n",
    "            pairs = arr.reshape(-1, 2)\n",
    "            print(\"pairs[0:5]:\\n\", pairs[:5])\n",
    "            lens = pairs[:5,1] - pairs[:5,0]\n",
    "            print(\"lens[0:5]:\", lens.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c0decdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video elems: 20339712\n",
      "\n",
      "=== BEST CANDIDATE ===\n",
      "No plausible interpretation found. (segment file might be compressed/encoded differently)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# video の dtype は spec に合わせて（仮に int32 とします）\n",
    "video = np.memmap(vid_path, mode=\"r\", dtype=np.int32)\n",
    "V = video.size\n",
    "\n",
    "TOK_PER_SAMPLE = 8 * 8 * 8  # 6144（想定）\n",
    "print(\"video elems:\", V)\n",
    "\n",
    "dtype_cands = [np.int64, np.uint64, np.int32, np.uint32, np.int16, np.uint16]\n",
    "offset_cands = [0, 4, 8, 12, 16, 24, 32, 64, 128, 256, 512]\n",
    "\n",
    "def score_as_pairs(arr):\n",
    "    \"\"\"arr を (N,2) の start/end として解釈した時の “それっぽさ” をスコア化\"\"\"\n",
    "    if arr.size < 10 or arr.size % 2 != 0:\n",
    "        return -1e9, None\n",
    "    pairs = arr.reshape(-1, 2).astype(np.int64)\n",
    "    s = pairs[:,0]\n",
    "    e = pairs[:,1]\n",
    "    # 基本条件\n",
    "    if np.any(e < s):\n",
    "        return -1e9, pairs\n",
    "    # 0長は減点\n",
    "    lens = e - s\n",
    "    nz = np.mean(lens > 0)\n",
    "    # 範囲内率\n",
    "    inb = np.mean((s >= 0) & (e <= V))\n",
    "    # 単調性（だいたい増えるはず）\n",
    "    mono = np.mean(np.diff(s) >= 0) if len(s) > 1 else 0.0\n",
    "    # 期待長 6144 に近い割合\n",
    "    near = np.mean(np.abs(lens - TOK_PER_SAMPLE) <= 8)  # 多少のズレ許容\n",
    "    # スコア（重みは経験則）\n",
    "    score = 3*inb + 2*mono + 2*near + 1*nz\n",
    "    return score, pairs\n",
    "\n",
    "def score_as_boundaries(arr):\n",
    "    \"\"\"arr を境界点 [b0,b1,...] として解釈して pairs にした時のスコア\"\"\"\n",
    "    if arr.size < 10:\n",
    "        return -1e9, None\n",
    "    b = arr.astype(np.int64)\n",
    "    # 境界は単調増加が自然\n",
    "    mono = np.mean(np.diff(b) >= 0)\n",
    "    if mono < 0.9:\n",
    "        return -1e9, None\n",
    "    s = b[:-1]\n",
    "    e = b[1:]\n",
    "    lens = e - s\n",
    "    nz = np.mean(lens > 0)\n",
    "    inb = np.mean((s >= 0) & (e <= V))\n",
    "    near = np.mean(np.abs(lens - TOK_PER_SAMPLE) <= 8)\n",
    "    score = 3*inb + 2*mono + 2*near + 1*nz\n",
    "    pairs = np.stack([s, e], axis=1)\n",
    "    return score, pairs\n",
    "\n",
    "best = None\n",
    "\n",
    "for off in offset_cands:\n",
    "    for dt in dtype_cands:\n",
    "        size = os.path.getsize(seg_path)\n",
    "        if size <= off:\n",
    "            continue\n",
    "        rem = size - off\n",
    "        if rem % np.dtype(dt).itemsize != 0:\n",
    "            continue\n",
    "\n",
    "        arr = np.memmap(seg_path, mode=\"r\", dtype=dt, offset=off)\n",
    "        arr = np.asarray(arr)\n",
    "\n",
    "        # 先頭が全部同じ、みたいなのは早期スキップ\n",
    "        if arr.size > 100 and np.all(arr[:100] == arr[0]):\n",
    "            continue\n",
    "\n",
    "        sp, pairs_p = score_as_pairs(arr)\n",
    "        sb, pairs_b = score_as_boundaries(arr)\n",
    "\n",
    "        for mode, sc, pairs in [(\"pairs\", sp, pairs_p), (\"boundaries\", sb, pairs_b)]:\n",
    "            if pairs is None:\n",
    "                continue\n",
    "            # 先頭5のレンズを見て「0長ばっか」は落とす\n",
    "            lens0 = (pairs[:5,1] - pairs[:5,0])\n",
    "            if np.mean(lens0 == 0) > 0.6:\n",
    "                continue\n",
    "\n",
    "            cand = (sc, mode, dt, off, pairs)\n",
    "            if best is None or cand[0] > best[0]:\n",
    "                best = cand\n",
    "\n",
    "print(\"\\n=== BEST CANDIDATE ===\")\n",
    "if best is None:\n",
    "    print(\"No plausible interpretation found. (segment file might be compressed/encoded differently)\")\n",
    "else:\n",
    "    sc, mode, dt, off, pairs = best\n",
    "    print(\"score:\", sc, \"mode:\", mode, \"dtype:\", dt, \"offset:\", off)\n",
    "    print(\"pairs[0:5]:\\n\", pairs[:5])\n",
    "    print(\"lens[0:5]:\", (pairs[:5,1] - pairs[:5,0]).tolist())\n",
    "    print(\"in-bounds ratio:\", np.mean((pairs[:,0] >= 0) & (pairs[:,1] <= V)))\n",
    "    print(\"near 6144 ratio:\", np.mean(np.abs((pairs[:,1]-pairs[:,0]) - TOK_PER_SAMPLE) <= 8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b462f95",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/root/work/data/raw/train_v2.0/metadata_0.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# load metadata.json\u001b[39;00m\n\u001b[32m     11\u001b[39m metadata = json.load(\u001b[38;5;28mopen\u001b[39m(dir_path / \u001b[33m\"\u001b[39m\u001b[33mmetadata.json\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m metadata_shard = json.load(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdir_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrank\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     14\u001b[39m total_frames = metadata_shard[\u001b[33m\"\u001b[39m\u001b[33mshard_num_frames\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     17\u001b[39m maps = [\n\u001b[32m     18\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33msegment_idx\u001b[39m\u001b[33m\"\u001b[39m, np.int32, []),\n\u001b[32m     19\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mstates\u001b[39m\u001b[33m\"\u001b[39m, np.float32, [\u001b[32m25\u001b[39m]),\n\u001b[32m     20\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:344\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    339\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/root/work/data/raw/train_v2.0/metadata_0.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pathlib\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "dir_path = pathlib.Path(\"/root/work/data/raw/train_v2.0\")\n",
    "rank = 0\n",
    "\n",
    "# load metadata.json\n",
    "metadata = json.load(open(dir_path / \"metadata.json\"))\n",
    "metadata_shard = json.load(open(dir_path / f\"metadata_{rank}.json\"))\n",
    "\n",
    "total_frames = metadata_shard[\"shard_num_frames\"]\n",
    "\n",
    "\n",
    "maps = [\n",
    "    (\"segment_idx\", np.int32, []),\n",
    "    (\"states\", np.float32, [25]),\n",
    "]\n",
    "\n",
    "for m, dtype, shape in maps:\n",
    "    filename = dir_path / f\"{m}_{rank}.bin\"\n",
    "    print(\"Reading\", filename, [total_frames] + shape)\n",
    "    m_out = np.memmap(filename, dtype=dtype, mode=\"r\", shape=tuple([total_frames] + shape))\n",
    "    assert m_out.shape[0] == total_frames\n",
    "    print(m, m_out[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "199a2163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_v2_sharded_fixed.py\n",
    "from __future__ import annotations\n",
    "import os, json, glob\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "@dataclass\n",
    "class V2Spec:\n",
    "    # sample definition\n",
    "    token_len: int = 8          # 3 past + 3 future\n",
    "    state_len: int = 64         # Revontuli-style (can set to 6 if you want)\n",
    "    H: int = 8\n",
    "    W: int = 8\n",
    "\n",
    "    # dtypes\n",
    "    video_dtype = np.int32\n",
    "    seg_dtype   = np.int32\n",
    "    state_dtype = np.float32\n",
    "\n",
    "class ShardedCompressionV2Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    v2.0 sharded format (segment_idx is per-frame segment ID).\n",
    "\n",
    "    Files per shard:\n",
    "      video_{rank}.bin       : (total_frames, H, W) int32 tokens\n",
    "      segment_idx_{rank}.bin : (total_frames,) int32 segment id per frame\n",
    "      states_{rank}.bin      : (total_frames, 25) float32\n",
    "      metadata_{rank}.json   : contains shard_num_frames\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir: str, split: str, spec: Optional[V2Spec] = None, use_memmap: bool = True):\n",
    "        super().__init__()\n",
    "        self.root = os.path.join(root_dir, split)\n",
    "        self.spec = spec or V2Spec()\n",
    "        self.use_memmap = use_memmap\n",
    "\n",
    "        # shard file lists\n",
    "        self.video_files = sorted(glob.glob(os.path.join(self.root, \"video_*.bin\")))\n",
    "        self.seg_files   = sorted(glob.glob(os.path.join(self.root, \"segment_idx_*.bin\")))\n",
    "        self.state_files = sorted(glob.glob(os.path.join(self.root, \"states_*.bin\")))\n",
    "        self.meta_files  = sorted(glob.glob(os.path.join(self.root, \"metadata_*.json\")))\n",
    "\n",
    "        assert len(self.video_files) > 0, f\"No video_*.bin under {self.root}\"\n",
    "        assert len(self.seg_files) == len(self.video_files), \"segment_idx shard count mismatch\"\n",
    "        assert len(self.state_files) == len(self.video_files), \"states shard count mismatch\"\n",
    "\n",
    "        # load shard_num_frames per shard\n",
    "        self.shard_frames: List[int] = []\n",
    "        for sid in range(len(self.video_files)):\n",
    "            meta_path = os.path.join(self.root, f\"metadata_{sid}.json\")\n",
    "            if not os.path.exists(meta_path):\n",
    "                raise FileNotFoundError(f\"Missing {meta_path}\")\n",
    "            meta = json.load(open(meta_path, \"r\"))\n",
    "            self.shard_frames.append(int(meta[\"shard_num_frames\"]))\n",
    "\n",
    "        # open memmaps + build index of valid windows\n",
    "        self.video_mmaps = []\n",
    "        self.seg_mmaps = []\n",
    "        self.state_mmaps = []\n",
    "\n",
    "        self.index: List[Tuple[int, int]] = []  # list of (sid, t0)\n",
    "\n",
    "        Ttok = self.spec.token_len\n",
    "        Tst  = self.spec.state_len\n",
    "        H, W = self.spec.H, self.spec.W\n",
    "\n",
    "        for sid, total_frames in enumerate(self.shard_frames):\n",
    "            vpath = self.video_files[sid]\n",
    "            gpath = self.seg_files[sid]\n",
    "            spath = self.state_files[sid]\n",
    "\n",
    "            # memmap with known shapes (from metadata: total_frames)\n",
    "            video = np.memmap(vpath, mode=\"r\", dtype=self.spec.video_dtype,\n",
    "                              shape=(total_frames, H, W))\n",
    "            segid = np.memmap(gpath, mode=\"r\", dtype=self.spec.seg_dtype,\n",
    "                              shape=(total_frames,))\n",
    "            states = np.memmap(spath, mode=\"r\", dtype=self.spec.state_dtype,\n",
    "                               shape=(total_frames, 25))\n",
    "\n",
    "            self.video_mmaps.append(video)\n",
    "            self.seg_mmaps.append(segid)\n",
    "            self.state_mmaps.append(states)\n",
    "\n",
    "            # ---- build segment boundaries from segid changes ----\n",
    "            # segments are runs of equal segid\n",
    "            segid_arr = np.asarray(segid)  # small enough to materialize for one shard\n",
    "            # boundary indices: where segid changes\n",
    "            change = np.nonzero(segid_arr[1:] != segid_arr[:-1])[0] + 1\n",
    "            # segment start indices\n",
    "            starts = np.concatenate([[0], change])\n",
    "            # segment end indices (exclusive)\n",
    "            ends = np.concatenate([change, [total_frames]])\n",
    "\n",
    "            # For each segment, allow windows t0 such that:\n",
    "            #   t0 + token_len <= seg_end\n",
    "            #   t0 + state_len <= seg_end   (to avoid crossing segment boundary)\n",
    "            # and also within total_frames.\n",
    "            for a, b in zip(starts, ends):\n",
    "                seg_len = b - a\n",
    "                # need at least max(token_len, state_len) frames in segment\n",
    "                need = max(Ttok, Tst)\n",
    "                if seg_len < need:\n",
    "                    continue\n",
    "                # valid t0 range: [a, b-need]\n",
    "                for t0 in range(a, b - need + 1):\n",
    "                    self.index.append((sid, int(t0)))\n",
    "\n",
    "        assert len(self.index) > 0, \"No valid samples found. Maybe state_len too large or H/W wrong?\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        sid, t0 = self.index[idx]\n",
    "        Ttok = self.spec.token_len\n",
    "        Tst  = self.spec.state_len\n",
    "\n",
    "        video = self.video_mmaps[sid]\n",
    "        states = self.state_mmaps[sid]\n",
    "        segid = self.seg_mmaps[sid]\n",
    "\n",
    "        # Safety: ensure we are not crossing segment boundary (should already be guaranteed)\n",
    "        seg0 = int(segid[t0])\n",
    "        if int(segid[t0 + max(Ttok, Tst) - 1]) != seg0:\n",
    "            raise ValueError(f\"Window crosses segment boundary unexpectedly: sid={sid}, t0={t0}\")\n",
    "\n",
    "        z = np.asarray(video[t0:t0 + Ttok])         # (6,H,W)\n",
    "        s = np.asarray(states[t0:t0 + Tst])         # (64,25)\n",
    "        \n",
    "        z = torch.from_numpy(np.array(z, copy=True)).long()\n",
    "        s = torch.from_numpy(np.array(s, copy=True)).float()\n",
    "\n",
    "        return z, s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85007278",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "No video_*.bin under /root/work/data/raw/train_v2.0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m spec = V2Spec(\n\u001b[32m      2\u001b[39m     token_len=\u001b[32m8\u001b[39m,\n\u001b[32m      3\u001b[39m     state_len=\u001b[32m64\u001b[39m,   \u001b[38;5;66;03m# 6にしたければここを6へ\u001b[39;00m\n\u001b[32m      4\u001b[39m     H=\u001b[32m8\u001b[39m, W=\u001b[32m8\u001b[39m,\n\u001b[32m      5\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m ds = \u001b[43mShardedCompressionV2Dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/root/work/data/raw/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain_v2.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_memmap\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m z, s = ds[\u001b[32m0\u001b[39m]\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mz:\u001b[39m\u001b[33m\"\u001b[39m, z.shape, z.dtype, \u001b[38;5;28mint\u001b[39m(z.min()), \u001b[38;5;28mint\u001b[39m(z.max()))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mShardedCompressionV2Dataset.__init__\u001b[39m\u001b[34m(self, root_dir, split, spec, use_memmap)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mself\u001b[39m.state_files = \u001b[38;5;28msorted\u001b[39m(glob.glob(os.path.join(\u001b[38;5;28mself\u001b[39m.root, \u001b[33m\"\u001b[39m\u001b[33mstates_*.bin\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m     45\u001b[39m \u001b[38;5;28mself\u001b[39m.meta_files  = \u001b[38;5;28msorted\u001b[39m(glob.glob(os.path.join(\u001b[38;5;28mself\u001b[39m.root, \u001b[33m\"\u001b[39m\u001b[33mmetadata_*.json\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.video_files) > \u001b[32m0\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo video_*.bin under \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.root\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.seg_files) == \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.video_files), \u001b[33m\"\u001b[39m\u001b[33msegment_idx shard count mismatch\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.state_files) == \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.video_files), \u001b[33m\"\u001b[39m\u001b[33mstates shard count mismatch\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: No video_*.bin under /root/work/data/raw/train_v2.0"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "spec = V2Spec(\n",
    "    token_len=8,\n",
    "    state_len=64,   # 6にしたければここを6へ\n",
    "    H=8, W=8,\n",
    ")\n",
    "\n",
    "ds = ShardedCompressionV2Dataset(\n",
    "    root_dir=\"/root/work/data/raw/\",\n",
    "    split=\"train_v2.0\",\n",
    "    spec=spec,\n",
    "    use_memmap=True,\n",
    ")\n",
    "\n",
    "z, s = ds[0]\n",
    "print(\"z:\", z.shape, z.dtype, int(z.min()), int(z.max()))\n",
    "print(\"s:\", s.shape, s.dtype, float(s.mean()), float(s.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e518037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_v2_flexible.py\n",
    "from __future__ import annotations\n",
    "import os, json, glob, re\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class V2SpecFlex:\n",
    "    token_len: int = 8      # DV8×8×8\n",
    "    state_len: int = 64\n",
    "    H: int = 8\n",
    "    W: int = 8\n",
    "    state_dim: int = 25\n",
    "\n",
    "    video_dtype = np.int32\n",
    "    seg_dtype   = np.int32\n",
    "    state_dtype = np.float32\n",
    "\n",
    "    allow_no_segment_idx: bool = True\n",
    "\n",
    "\n",
    "def _load_json(path: str) -> Dict:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def _find_overall_metadata_json(split_dir: str) -> Optional[str]:\n",
    "    p = os.path.join(split_dir, \"metadata.json\")\n",
    "    return p if os.path.exists(p) else None\n",
    "\n",
    "\n",
    "def _find_per_shard_metadata(split_dir: str) -> Dict[int, str]:\n",
    "    cand = []\n",
    "    cand += glob.glob(os.path.join(split_dir, \"metadata\", \"metadata_*.json\"))\n",
    "    cand += glob.glob(os.path.join(split_dir, \"metadata_*.json\"))\n",
    "    out = {}\n",
    "    for p in cand:\n",
    "        m = re.search(r\"metadata_(\\d+)\\.json$\", p)\n",
    "        if m:\n",
    "            out[int(m.group(1))] = p\n",
    "    return dict(sorted(out.items(), key=lambda x: x[0]))\n",
    "\n",
    "\n",
    "def _find_videos(split_dir: str) -> Dict[int, str]:\n",
    "    patterns = [\n",
    "        os.path.join(split_dir, \"segment_indices\", \"videos\", \"video_*.bin\"),\n",
    "        os.path.join(split_dir, \"videos\", \"video_*.bin\"),\n",
    "        os.path.join(split_dir, \"video_*.bin\"),\n",
    "    ]\n",
    "    files = []\n",
    "    for pat in patterns:\n",
    "        files += glob.glob(pat)\n",
    "\n",
    "    out = {}\n",
    "    for p in files:\n",
    "        m = re.search(r\"video_(\\d+)\\.bin$\", p)\n",
    "        if m:\n",
    "            out[int(m.group(1))] = p\n",
    "    return dict(sorted(out.items(), key=lambda x: x[0]))\n",
    "\n",
    "\n",
    "def _find_states(split_dir: str) -> Dict[int, str]:\n",
    "    patterns = [\n",
    "        os.path.join(split_dir, \"robot_states\", \"states_*.bin\"),\n",
    "        os.path.join(split_dir, \"states_*.bin\"),\n",
    "    ]\n",
    "    files = []\n",
    "    for pat in patterns:\n",
    "        files += glob.glob(pat)\n",
    "\n",
    "    out = {}\n",
    "    for p in files:\n",
    "        m = re.search(r\"states_(\\d+)\\.bin$\", p)\n",
    "        if m:\n",
    "            out[int(m.group(1))] = p\n",
    "    return dict(sorted(out.items(), key=lambda x: x[0]))\n",
    "\n",
    "\n",
    "def _find_segment_idx(split_dir: str) -> Dict[int, str]:\n",
    "    patterns = [\n",
    "        os.path.join(split_dir, \"segment_indices\", \"segment_idx_*.bin\"),\n",
    "        os.path.join(split_dir, \"segment_idx_*.bin\"),\n",
    "        os.path.join(split_dir, \"segment_idx_*.json\"),\n",
    "    ]\n",
    "    files = []\n",
    "    for pat in patterns:\n",
    "        files += glob.glob(pat)\n",
    "\n",
    "    out = {}\n",
    "    for p in files:\n",
    "        m = re.search(r\"segment_idx_(\\d+)\\.(?:bin|json)$\", p)\n",
    "        if m:\n",
    "            out[int(m.group(1))] = p\n",
    "    return dict(sorted(out.items(), key=lambda x: x[0]))\n",
    "\n",
    "\n",
    "def _load_segment_idx(path: str, total_frames: int, dtype=np.int32) -> np.ndarray:\n",
    "    if path.endswith(\".bin\"):\n",
    "        seg = np.memmap(path, mode=\"r\", dtype=dtype, shape=(total_frames,))\n",
    "        return np.asarray(seg)\n",
    "    elif path.endswith(\".json\"):\n",
    "        obj = _load_json(path)\n",
    "        if isinstance(obj, list):\n",
    "            seg = np.array(obj, dtype=dtype)\n",
    "        elif isinstance(obj, dict) and \"segment_idx\" in obj:\n",
    "            seg = np.array(obj[\"segment_idx\"], dtype=dtype)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown segment_idx json format: {path}\")\n",
    "        if seg.shape[0] != total_frames:\n",
    "            raise ValueError(\"segment_idx length mismatch\")\n",
    "        return seg\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported segment_idx file: {path}\")\n",
    "\n",
    "def _infer_total_frames_from_file_sizes(\n",
    "    video_path: str,\n",
    "    states_path: str,\n",
    "    H: int,\n",
    "    W: int,\n",
    "    state_dim: int,\n",
    "    video_dtype=np.int32,\n",
    "    state_dtype=np.float32,\n",
    ") -> int:\n",
    "    v_bytes = os.path.getsize(video_path)\n",
    "    s_bytes = os.path.getsize(states_path)\n",
    "\n",
    "    v_item = np.dtype(video_dtype).itemsize\n",
    "    s_item = np.dtype(state_dtype).itemsize\n",
    "\n",
    "    # video: (T, H, W)\n",
    "    denom_v = H * W * v_item\n",
    "    if v_bytes % denom_v != 0:\n",
    "        raise ValueError(f\"video size not divisible: {video_path} bytes={v_bytes}, denom={denom_v}\")\n",
    "    Tv = v_bytes // denom_v\n",
    "\n",
    "    # states: (T, state_dim)\n",
    "    denom_s = state_dim * s_item\n",
    "    if s_bytes % denom_s != 0:\n",
    "        raise ValueError(f\"states size not divisible: {states_path} bytes={s_bytes}, denom={denom_s}\")\n",
    "    Ts = s_bytes // denom_s\n",
    "\n",
    "    if Tv != Ts:\n",
    "        raise ValueError(f\"frame count mismatch: video={Tv}, states={Ts} for shard files:\\n{video_path}\\n{states_path}\")\n",
    "    return int(Tv)\n",
    "\n",
    "class ShardedCompressionV2Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Flexible loader for train_v2.0 / val_v2.0 / test_v2.0 with different folder layouts.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir: str, split: str, spec: Optional[V2SpecFlex] = None, use_memmap: bool = True):\n",
    "        super().__init__()\n",
    "        self.split_dir = os.path.join(root_dir, split)\n",
    "        self.spec = spec or V2SpecFlex()\n",
    "        self.use_memmap = use_memmap\n",
    "\n",
    "        if not os.path.isdir(self.split_dir):\n",
    "            raise FileNotFoundError(f\"Split dir not found: {self.split_dir}\")\n",
    "\n",
    "        # discover files\n",
    "        meta_overall = _find_overall_metadata_json(self.split_dir)\n",
    "        self.meta_overall = _load_json(meta_overall) if meta_overall else None\n",
    "\n",
    "        self.meta_shard = _find_per_shard_metadata(self.split_dir)\n",
    "        self.videos = _find_videos(self.split_dir)\n",
    "        self.states = _find_states(self.split_dir)\n",
    "        self.segment_idx = _find_segment_idx(self.split_dir)\n",
    "\n",
    "        # determine shard ids we can load\n",
    "        # shard ids we can load:\n",
    "        # - train/val: require metadata_shard\n",
    "        # - test: allow no metadata_shard (infer from file sizes)\n",
    "        common_vs = sorted(set(self.videos.keys()) & set(self.states.keys()))\n",
    "\n",
    "        if len(common_vs) == 0:\n",
    "            raise ValueError(\"No matching shard ids between video and states files.\")\n",
    "\n",
    "        # Prefer shards that have metadata, but allow shards without it (test)\n",
    "        self.shard_ids = common_vs\n",
    "\n",
    "        # store total_frames per shard\n",
    "        self.total_frames_by_shard = {}\n",
    "\n",
    "        for sid in self.shard_ids:\n",
    "            meta_path = self.meta_shard.get(sid, None)\n",
    "            if meta_path is not None:\n",
    "                meta = _load_json(meta_path)\n",
    "                total_frames = int(meta.get(\"shard_num_frames\", meta.get(\"num_frames\", -1)))\n",
    "                if total_frames <= 0:\n",
    "                    raise ValueError(f\"metadata_{sid}.json missing shard_num_frames: {meta_path}\")\n",
    "            else:\n",
    "                # ★ metadata 無し（test想定）：ファイルサイズから推定\n",
    "                total_frames = _infer_total_frames_from_file_sizes(\n",
    "                    video_path=self.videos[sid],\n",
    "                    states_path=self.states[sid],\n",
    "                    H=self.spec.H,\n",
    "                    W=self.spec.W,\n",
    "                    state_dim=self.spec.state_dim,\n",
    "                    video_dtype=self.spec.video_dtype,\n",
    "                    state_dtype=self.spec.state_dtype,\n",
    "                )\n",
    "\n",
    "            self.total_frames_by_shard[sid] = total_frames\n",
    "\n",
    "        # open memmaps + build index\n",
    "        self.video_mmaps = {}\n",
    "        self.state_mmaps = {}\n",
    "        self.seg_arrays = {}  # may be missing in test\n",
    "        self.index: List[Tuple[int, int]] = []  # (shard_id, t0)\n",
    "\n",
    "        Ttok = self.spec.token_len\n",
    "        Tst  = self.spec.state_len\n",
    "        H, W = self.spec.H, self.spec.W\n",
    "        Sd   = self.spec.state_dim\n",
    "\n",
    "        for sid in self.shard_ids:\n",
    "            meta = _load_json(self.meta_shard[sid])\n",
    "            total_frames = self.total_frames_by_shard[sid]\n",
    "            if total_frames <= 0:\n",
    "                raise ValueError(f\"metadata_{sid}.json missing shard_num_frames: {self.meta_shard[sid]}\")\n",
    "\n",
    "            # video\n",
    "            vpath = self.videos[sid]\n",
    "            video = np.memmap(vpath, mode=\"r\", dtype=self.spec.video_dtype, shape=(total_frames, H, W))\n",
    "            self.video_mmaps[sid] = video\n",
    "\n",
    "            # states\n",
    "            spath = self.states[sid]\n",
    "            states = np.memmap(spath, mode=\"r\", dtype=self.spec.state_dtype, shape=(total_frames, Sd))\n",
    "            self.state_mmaps[sid] = states\n",
    "\n",
    "            # segment_idx (optional in test)\n",
    "            if sid in self.segment_idx:\n",
    "                seg = _load_segment_idx(self.segment_idx[sid], total_frames=total_frames, dtype=self.spec.seg_dtype)\n",
    "                self.seg_arrays[sid] = seg\n",
    "                # build segment boundaries from seg id changes\n",
    "                change = np.nonzero(seg[1:] != seg[:-1])[0] + 1\n",
    "                starts = np.concatenate([[0], change])\n",
    "                ends   = np.concatenate([change, [total_frames]])\n",
    "                need = max(Ttok, Tst)\n",
    "                for a, b in zip(starts, ends):\n",
    "                    if (b - a) < need:\n",
    "                        continue\n",
    "                    for t0 in range(int(a), int(b - need + 1)):\n",
    "                        self.index.append((sid, t0))\n",
    "            else:\n",
    "                if not self.spec.allow_no_segment_idx:\n",
    "                    continue\n",
    "                # fallback: treat whole shard as one segment\n",
    "                need = max(Ttok, Tst)\n",
    "                if total_frames < need:\n",
    "                    continue\n",
    "                for t0 in range(0, total_frames - need + 1):\n",
    "                    self.index.append((sid, int(t0)))\n",
    "\n",
    "        if len(self.index) == 0:\n",
    "            raise ValueError(\"No valid samples built. Maybe token_len/state_len too large or shapes wrong.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        sid, t0 = self.index[idx]\n",
    "        Ttok = self.spec.token_len\n",
    "        Tst  = self.spec.state_len\n",
    "\n",
    "        video = self.video_mmaps[sid]\n",
    "        states = self.state_mmaps[sid]\n",
    "\n",
    "        # Safety: if segment_idx exists, ensure window stays in same segment\n",
    "        if sid in self.seg_arrays:\n",
    "            seg = self.seg_arrays[sid]\n",
    "            seg0 = int(seg[t0])\n",
    "            if int(seg[t0 + max(Ttok, Tst) - 1]) != seg0:\n",
    "                raise ValueError(f\"Crossed segment boundary: shard={sid} t0={t0}\")\n",
    "\n",
    "        z = np.asarray(video[t0:t0 + Ttok])          # (Ttok,H,W)\n",
    "        s = np.asarray(states[t0:t0 + Tst])          # (Tst,25)\n",
    "\n",
    "        # avoid non-writable memmap warning\n",
    "        z = torch.from_numpy(z.copy()).long()\n",
    "        s = torch.from_numpy(s.copy()).float()\n",
    "        return z, s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c82edb67",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "frame count mismatch: video=48, states=64 for shard files:\n/root/work/data/raw/test_v2.0/videos/video_0.bin\n/root/work/data/raw/test_v2.0/robot_states/states_0.bin",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m train_ds = ShardedCompressionV2Dataset(\u001b[33m\"\u001b[39m\u001b[33m/root/work/data/raw\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtrain_v2.0\u001b[39m\u001b[33m\"\u001b[39m, spec=spec)\n\u001b[32m      8\u001b[39m val_ds   = ShardedCompressionV2Dataset(\u001b[33m\"\u001b[39m\u001b[33m/root/work/data/raw\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mval_v2.0\u001b[39m\u001b[33m\"\u001b[39m,   spec=spec)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m test_ds  = \u001b[43mShardedCompressionV2Dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/root/work/data/raw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest_v2.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# segment_idx無しでも動く\u001b[39;00m\n\u001b[32m     11\u001b[39m z, s = train_ds[\u001b[32m0\u001b[39m]\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtrain sample:\u001b[39m\u001b[33m\"\u001b[39m, z.shape, s.shape)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 197\u001b[39m, in \u001b[36mShardedCompressionV2Dataset.__init__\u001b[39m\u001b[34m(self, root_dir, split, spec, use_memmap)\u001b[39m\n\u001b[32m    194\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmetadata_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json missing shard_num_frames: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeta_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    196\u001b[39m         \u001b[38;5;66;03m# ★ metadata 無し（test想定）：ファイルサイズから推定\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         total_frames = \u001b[43m_infer_total_frames_from_file_sizes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvideos\u001b[49m\u001b[43m[\u001b[49m\u001b[43msid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstates_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m[\u001b[49m\u001b[43msid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m            \u001b[49m\u001b[43mH\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m            \u001b[49m\u001b[43mW\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstate_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvideo_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvideo_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstate_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m.total_frames_by_shard[sid] = total_frames\n\u001b[32m    209\u001b[39m \u001b[38;5;66;03m# open memmaps + build index\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 148\u001b[39m, in \u001b[36m_infer_total_frames_from_file_sizes\u001b[39m\u001b[34m(video_path, states_path, H, W, state_dim, video_dtype, state_dtype)\u001b[39m\n\u001b[32m    145\u001b[39m Ts = s_bytes // denom_s\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m Tv != Ts:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mframe count mismatch: video=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, states=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for shard files:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvideo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstates_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(Tv)\n",
      "\u001b[31mValueError\u001b[39m: frame count mismatch: video=48, states=64 for shard files:\n/root/work/data/raw/test_v2.0/videos/video_0.bin\n/root/work/data/raw/test_v2.0/robot_states/states_0.bin"
     ]
    }
   ],
   "source": [
    "spec = V2SpecFlex(\n",
    "    token_len=8,  # DV8×8×8\n",
    "    state_len=64,\n",
    "    H=8, W=8,\n",
    ")\n",
    "\n",
    "train_ds = ShardedCompressionV2Dataset(\"/root/work/data/raw\", \"train_v2.0\", spec=spec)\n",
    "val_ds   = ShardedCompressionV2Dataset(\"/root/work/data/raw\", \"val_v2.0\",   spec=spec)\n",
    "test_ds  = ShardedCompressionV2Dataset(\"/root/work/data/raw\", \"test_v2.0\",  spec=spec)  # segment_idx無しでも動く\n",
    "\n",
    "z, s = train_ds[0]\n",
    "print(\"train sample:\", z.shape, s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb601f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video bytes: 12288 itemsize(int32): 4\n",
      "states bytes: 6400 itemsize(float32): 4\n",
      "video n_int32: 3072 remainder: 0\n",
      "states n_f32 : 1600 remainder: 0\n",
      "video == 8*8*8 ? False\n",
      "states == 64*25 ? True\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np\n",
    "\n",
    "vpath = \"/root/work/data/raw/test_v2.0/videos/video_0.bin\"\n",
    "spath = \"/root/work/data/raw/test_v2.0/robot_states/states_0.bin\"\n",
    "\n",
    "v_bytes = os.path.getsize(vpath)\n",
    "s_bytes = os.path.getsize(spath)\n",
    "\n",
    "print(\"video bytes:\", v_bytes, \"itemsize(int32):\", np.dtype(np.int32).itemsize)\n",
    "print(\"states bytes:\", s_bytes, \"itemsize(float32):\", np.dtype(np.float32).itemsize)\n",
    "\n",
    "print(\"video n_int32:\", v_bytes // 4, \"remainder:\", v_bytes % 4)\n",
    "print(\"states n_f32 :\", s_bytes // 4, \"remainder:\", s_bytes % 4)\n",
    "\n",
    "# 期待候補\n",
    "print(\"video == 8*8*8 ?\", (v_bytes//4) == (8*8*8))\n",
    "print(\"states == 64*25 ?\", (s_bytes//4) == (64*25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c2ea5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytes: 81358848\n",
      "int32 elems: 20339712\n",
      "div by 3072 ? True\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np\n",
    "\n",
    "vpath = \"/root/work/data/raw/train_v2.0/segment_indices/videos/video_0.bin\"\n",
    "bytes = os.path.getsize(vpath)\n",
    "print(\"bytes:\", bytes)\n",
    "print(\"int32 elems:\", bytes // 4)\n",
    "\n",
    "# 32*32*3 = 3072\n",
    "print(\"div by 3072 ?\", (bytes // 4) % 3072 == 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b53df369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_frames: 112542\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "mmap length is greater than file size",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m T = \u001b[38;5;28mint\u001b[39m(meta[\u001b[33m\"\u001b[39m\u001b[33mshard_num_frames\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtotal_frames:\u001b[39m\u001b[33m\"\u001b[39m, T)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m video = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmemmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvid_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m segid = np.memmap(seg_path, mode=\u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, dtype=np.int32,   shape=(T,))\n\u001b[32m     19\u001b[39m state = np.memmap(st_path,  mode=\u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, dtype=np.float32, shape=(T, \u001b[32m25\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.12/site-packages/numpy/_core/memmap.py:291\u001b[39m, in \u001b[36mmemmap.__new__\u001b[39m\u001b[34m(subtype, filename, dtype, mode, offset, shape, order)\u001b[39m\n\u001b[32m    289\u001b[39m     start -= mmap.ALLOCATIONGRANULARITY\n\u001b[32m    290\u001b[39m array_offset = offset - start\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m mm = \u001b[43mmmap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfileno\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccess\u001b[49m\u001b[43m=\u001b[49m\u001b[43macc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[38;5;28mself\u001b[39m = ndarray.\u001b[34m__new__\u001b[39m(subtype, shape, dtype=descr, buffer=mm,\n\u001b[32m    294\u001b[39m                        offset=array_offset, order=order)\n\u001b[32m    295\u001b[39m \u001b[38;5;28mself\u001b[39m._mmap = mm\n",
      "\u001b[31mValueError\u001b[39m: mmap length is greater than file size"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "\n",
    "split_dir = \"/root/work/data/raw/train_v2.0\"\n",
    "rank = 0\n",
    "\n",
    "# train の構造に合わせてパスを調整（あなたの構造: segment_indices/videos/ と robot_states/ と metadata/）\n",
    "meta_path = os.path.join(split_dir, \"metadata\", f\"metadata_{rank}.json\")\n",
    "seg_path  = os.path.join(split_dir, \"segment_indices\", f\"segment_idx_{rank}.bin\")\n",
    "vid_path  = os.path.join(split_dir, \"segment_indices\", \"videos\", f\"video_{rank}.bin\")\n",
    "st_path   = os.path.join(split_dir, \"robot_states\", f\"states_{rank}.bin\")\n",
    "\n",
    "meta = json.load(open(meta_path))\n",
    "T = int(meta[\"shard_num_frames\"])\n",
    "print(\"total_frames:\", T)\n",
    "\n",
    "video = np.memmap(vid_path, mode=\"r\", dtype=np.int32,   shape=(T, 32, 32))\n",
    "segid = np.memmap(seg_path, mode=\"r\", dtype=np.int32,   shape=(T,))\n",
    "state = np.memmap(st_path,  mode=\"r\", dtype=np.float32, shape=(T, 25))\n",
    "\n",
    "print(\"video shape:\", video.shape, \"dtype:\", video.dtype)\n",
    "print(\"segid shape:\", segid.shape, \"dtype:\", segid.dtype)\n",
    "print(\"state shape:\", state.shape, \"dtype:\", state.dtype)\n",
    "\n",
    "# 先頭10の segid（変化しているか）\n",
    "print(\"segid head:\", np.asarray(segid[:20]).tolist())\n",
    "\n",
    "# 値域ざっくり\n",
    "v0 = np.asarray(video[0])\n",
    "print(\"video[0] min/max:\", int(v0.min()), int(v0.max()))\n",
    "print(\"state[0] mean/std:\", float(state[0].mean()), float(state[0].std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52e7066a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T(meta) = 112542\n",
      "video bytes: 81358848\n",
      "segment_idx bytes: 450168\n",
      "states bytes: 11254200\n",
      "seg elems int32: 112542 remainder: 0\n",
      "state elems f32 : 2813550 remainder: 0\n",
      "need video bytes if (T,32,32): 460972032\n",
      "need video bytes if (T, 8, 8): 28810752\n"
     ]
    }
   ],
   "source": [
    "import os, json, numpy as np\n",
    "\n",
    "split_dir = \"/root/work/data/raw/train_v2.0\"\n",
    "rank = 0\n",
    "\n",
    "meta_path = os.path.join(split_dir, \"metadata\", f\"metadata_{rank}.json\")\n",
    "seg_path  = os.path.join(split_dir, \"segment_indices\", f\"segment_idx_{rank}.bin\")\n",
    "vid_path  = os.path.join(split_dir, \"segment_indices\", \"videos\", f\"video_{rank}.bin\")\n",
    "st_path   = os.path.join(split_dir, \"robot_states\", f\"states_{rank}.bin\")\n",
    "\n",
    "meta = json.load(open(meta_path))\n",
    "T = int(meta[\"shard_num_frames\"])\n",
    "print(\"T(meta) =\", T)\n",
    "\n",
    "vid_bytes = os.path.getsize(vid_path)\n",
    "seg_bytes = os.path.getsize(seg_path)\n",
    "st_bytes  = os.path.getsize(st_path)\n",
    "\n",
    "print(\"video bytes:\", vid_bytes)\n",
    "print(\"segment_idx bytes:\", seg_bytes)\n",
    "print(\"states bytes:\", st_bytes)\n",
    "\n",
    "# sanity: segment_idx and states are per-frame arrays in v2.0\n",
    "print(\"seg elems int32:\", seg_bytes // 4, \"remainder:\", seg_bytes % 4)\n",
    "print(\"state elems f32 :\", st_bytes // 4,  \"remainder:\", st_bytes % 4)\n",
    "\n",
    "need_vid_32 = T * 32 * 32 * np.dtype(np.int32).itemsize\n",
    "need_vid_8  = T * 8  * 8  * np.dtype(np.int32).itemsize\n",
    "print(\"need video bytes if (T,32,32):\", need_vid_32)\n",
    "print(\"need video bytes if (T, 8, 8):\", need_vid_8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21609dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video int32 elems: 20339712\n",
      "video elems / T(meta): 180.7299674788079\n",
      "divisible by 32x32 (1024) -> inferred T=19863\n",
      "divisible by 3x32x32 (3072) -> inferred T=6621\n",
      "divisible by 8x8 (64) -> inferred T=317808\n",
      "divisible by 8x8x8 (512) -> inferred T=39726\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, os\n",
    "\n",
    "vid_elems = os.path.getsize(vid_path) // np.dtype(np.int32).itemsize\n",
    "print(\"video int32 elems:\", vid_elems)\n",
    "print(\"video elems / T(meta):\", vid_elems / T)\n",
    "\n",
    "# よくある候補を当てる\n",
    "cands = {\n",
    "    \"32x32\": 32*32,\n",
    "    \"3x32x32\": 3*32*32,\n",
    "    \"8x8\": 8*8,\n",
    "    \"8x8x8\": 8*8*8,  # 512\n",
    "    \"6x32x32\": 6*32*32,\n",
    "}\n",
    "for name, k in cands.items():\n",
    "    ok = (vid_elems % k == 0)\n",
    "    if ok:\n",
    "        t_infer = vid_elems // k\n",
    "        print(f\"divisible by {name} ({k}) -> inferred T={t_infer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d78595ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T(frames): 112542\n",
      "video elems: 20339712 => N_cubes(if 8x8x8): 39726 remainder: 0\n",
      "seg min/max: 0 520\n",
      "unique seg count: 521\n",
      "seg max < N_cubes ? True\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np, json\n",
    "\n",
    "split_dir = \"/root/work/data/raw/train_v2.0\"\n",
    "rank = 0\n",
    "meta = json.load(open(f\"{split_dir}/metadata/metadata_{rank}.json\"))\n",
    "T = int(meta[\"shard_num_frames\"])\n",
    "\n",
    "vid_path = f\"{split_dir}/segment_indices/videos/video_{rank}.bin\"\n",
    "seg_path = f\"{split_dir}/segment_indices/segment_idx_{rank}.bin\"\n",
    "\n",
    "vid_elems = os.path.getsize(vid_path)//4\n",
    "N = vid_elems//512\n",
    "print(\"T(frames):\", T)\n",
    "print(\"video elems:\", vid_elems, \"=> N_cubes(if 8x8x8):\", N, \"remainder:\", vid_elems % 512)\n",
    "\n",
    "seg = np.memmap(seg_path, mode=\"r\", dtype=np.int32, shape=(T,))\n",
    "seg = np.asarray(seg)\n",
    "print(\"seg min/max:\", int(seg.min()), int(seg.max()))\n",
    "print(\"unique seg count:\", int(np.unique(seg).size))\n",
    "print(\"seg max < N_cubes ?\", int(seg.max()) < N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ebe64c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
