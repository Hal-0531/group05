{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "397b61ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stwm_compression_template.py\n",
    "from __future__ import annotations\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from einops import rearrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d220d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "#  Utilities\n",
    "# -------------------------\n",
    "\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "class QKNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Query-Key normalization (Henry et al., 2020-ish idea).\n",
    "    Normalize q and k per head to stabilize attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # q,k: (B, heads, N, d)\n",
    "        q = q / (q.norm(dim=-1, keepdim=True) + self.eps)\n",
    "        k = k / (k.norm(dim=-1, keepdim=True) + self.eps)\n",
    "        return q, k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d4fb209",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional xFormers memory-efficient attention\n",
    "_HAS_XFORMERS = False\n",
    "try:\n",
    "    from xformers.ops import memory_efficient_attention\n",
    "    _HAS_XFORMERS = True\n",
    "except Exception:\n",
    "    _HAS_XFORMERS = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "924343c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiheadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal MHA with optional xformers.\n",
    "    Supports causal mask for temporal attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, heads: int, qk_norm: bool = True, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        assert dim % heads == 0\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        self.d = dim // heads\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(dim, dim, bias=False)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.qkn = QKNorm() if qk_norm else None\n",
    "\n",
    "    def forward(self, x: torch.Tensor, causal: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, N, C)\n",
    "        \"\"\"\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.to_qkv(x)  # (B, N, 3C)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # (B, heads, N, d)\n",
    "        q = rearrange(q, \"b n (h d) -> b h n d\", h=self.heads)\n",
    "        k = rearrange(k, \"b n (h d) -> b h n d\", h=self.heads)\n",
    "        v = rearrange(v, \"b n (h d) -> b h n d\", h=self.heads)\n",
    "\n",
    "        if self.qkn is not None:\n",
    "            q, k = self.qkn(q, k)\n",
    "\n",
    "        if _HAS_XFORMERS:\n",
    "            # xFormers expects (B, N, H, d) or (B, N, C) depending on op;\n",
    "            # memory_efficient_attention uses (B, N, H, d).\n",
    "            q_ = rearrange(q, \"b h n d -> b n h d\")\n",
    "            k_ = rearrange(k, \"b h n d -> b n h d\")\n",
    "            v_ = rearrange(v, \"b h n d -> b n h d\")\n",
    "\n",
    "            attn_bias = None\n",
    "            if causal:\n",
    "                # xFormers causal mask bias helper is not always available;\n",
    "                # simplest: fall back to torch attention if causal and xformers missing bias.\n",
    "                # But many xformers builds support a causal flag via an attention bias.\n",
    "                # To keep template robust, we handle causal in torch path below.\n",
    "                pass\n",
    "\n",
    "            if causal:\n",
    "                # Robust fallback (still fine because temporal N is small: T<=6)\n",
    "                return self._torch_attention(x, q, k, v, causal=True)\n",
    "\n",
    "            out = memory_efficient_attention(q_, k_, v_, attn_bias=attn_bias)  # (B, N, H, d)\n",
    "            out = rearrange(out, \"b n h d -> b n (h d)\")\n",
    "        else:\n",
    "            out = self._torch_attention(x, q, k, v, causal=causal)\n",
    "\n",
    "        out = self.proj(out)\n",
    "        out = self.drop(out)\n",
    "        return out\n",
    "\n",
    "    def _torch_attention(self, x, q, k, v, causal: bool) -> torch.Tensor:\n",
    "        # q,k,v: (B, heads, N, d)\n",
    "        B, H, N, d = q.shape\n",
    "        scale = 1.0 / math.sqrt(d)\n",
    "\n",
    "        # (B, H, N, N)\n",
    "        scores = torch.einsum(\"b h i d, b h j d -> b h i j\", q, k) * scale\n",
    "\n",
    "        if causal:\n",
    "            # causal mask: disallow attending to future positions\n",
    "            mask = torch.triu(torch.ones(N, N, device=x.device, dtype=torch.bool), diagonal=1)\n",
    "            scores = scores.masked_fill(mask, float(\"-inf\"))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        out = torch.einsum(\"b h i j, b h j d -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim: int, mult: int = 4, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        inner = dim * mult\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, inner),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(inner, dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9e58ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "#  ST-Transformer Block\n",
    "# -------------------------\n",
    "\n",
    "class STBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    One block: Spatial Attn -> Temporal (causal) Attn -> FFN\n",
    "    Pre-LN style.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, heads: int, dropout: float = 0.0, qk_norm: bool = True):\n",
    "        super().__init__()\n",
    "        self.ln_s = nn.LayerNorm(dim)\n",
    "        self.ln_t = nn.LayerNorm(dim)\n",
    "        self.ln_f = nn.LayerNorm(dim)\n",
    "\n",
    "        self.attn_s = MultiheadSelfAttention(dim, heads, qk_norm=qk_norm, dropout=dropout)\n",
    "        self.attn_t = MultiheadSelfAttention(dim, heads, qk_norm=qk_norm, dropout=dropout)\n",
    "        self.ff = FeedForward(dim, mult=4, dropout=dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, T: int, HW: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, T*HW, C)\n",
    "        We reshape for factorized attention:\n",
    "          - Spatial: for each t, attend over HW tokens\n",
    "          - Temporal: for each spatial index p, attend over T tokens (causal)\n",
    "        \"\"\"\n",
    "        B, N, C = x.shape\n",
    "        assert N == T * HW\n",
    "\n",
    "        # Spatial attention: (B*T, HW, C)\n",
    "        xs = rearrange(x, \"b (t p) c -> (b t) p c\", t=T, p=HW)\n",
    "        xs = xs + self.attn_s(self.ln_s(xs), causal=False)\n",
    "        x = rearrange(xs, \"(b t) p c -> b (t p) c\", b=B, t=T, p=HW)\n",
    "\n",
    "        # Temporal attention: (B*HW, T, C), causal\n",
    "        xt = rearrange(x, \"b (t p) c -> (b p) t c\", t=T, p=HW)\n",
    "        xt = xt + self.attn_t(self.ln_t(xt), causal=True)\n",
    "        x = rearrange(xt, \"(b p) t c -> b (t p) c\", b=B, p=HW, t=T)\n",
    "\n",
    "        # FFN\n",
    "        x = x + self.ff(self.ln_f(x))\n",
    "        return x\n",
    "\n",
    "# -------------------------\n",
    "#  State embedding (robot states)\n",
    "# -------------------------\n",
    "\n",
    "class StateEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    s: (B, 64, Sdim)\n",
    "    -> returns per-frame conditioning: (B, T, C)\n",
    "    Revontuli mentions MLP + Conv1d + pos emb + additive embedding.\n",
    "    Here: MLP -> Conv1d over time -> sample/align to T -> add to tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim: int, model_dim: int, conv_channels: int = 256):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(state_dim, conv_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(conv_channels, conv_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(conv_channels, conv_channels, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(conv_channels, conv_channels, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.to_model = nn.Linear(conv_channels, model_dim)\n",
    "\n",
    "    def forward(self, s: torch.Tensor, T: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        s: (B, 64, Sd)\n",
    "        returns: (B, T, C)\n",
    "        \"\"\"\n",
    "        B, L, Sd = s.shape  # L=64\n",
    "        h = self.mlp(s)  # (B, L, conv_channels)\n",
    "        h = rearrange(h, \"b l c -> b c l\")\n",
    "        h = self.conv(h)\n",
    "        h = rearrange(h, \"b c l -> b l c\")  # (B, L, conv_channels)\n",
    "\n",
    "        # Align length 64 -> T using interpolation (simple & robust)\n",
    "        if L != T:\n",
    "            # (B, C, L) -> (B, C, T)\n",
    "            h_ = rearrange(h, \"b l c -> b c l\")\n",
    "            h_ = F.interpolate(h_, size=T, mode=\"linear\", align_corners=False)\n",
    "            h = rearrange(h_, \"b c t -> b t c\")\n",
    "\n",
    "        cond = self.to_model(h)  # (B, T, model_dim)\n",
    "        return cond\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "597d8fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "#  Main model\n",
    "# -------------------------\n",
    "\n",
    "class RevontuliCompressionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Token dynamics model:\n",
    "      Input: past tokens (B, 3, 32, 32) + robot states (B, 64, 25)\n",
    "      Target: future tokens (B, 3, 32, 32)\n",
    "    Train with teacher forcing over all 6 frames (past+future),\n",
    "    but only compute CE on the future frames.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        model_dim: int = 512,\n",
    "        n_layers: int = 24,\n",
    "        n_heads: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        state_dim: int = 25,\n",
    "        H: int = 32,\n",
    "        W: int = 32,\n",
    "        n_past: int = 3,\n",
    "        n_future: int = 3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.model_dim = model_dim\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        self.HW = H * W\n",
    "        self.n_past = n_past\n",
    "        self.n_future = n_future\n",
    "        self.T_total = n_past + n_future  # 6\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, model_dim)\n",
    "        # Simple learned spatial position (HW) and temporal position (T)\n",
    "        self.pos_spatial = nn.Parameter(torch.zeros(1, 1, self.HW, model_dim))\n",
    "        self.pos_temporal = nn.Parameter(torch.zeros(1, self.T_total, 1, model_dim))\n",
    "\n",
    "        self.state_enc = StateEncoder(state_dim=state_dim, model_dim=model_dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            STBlock(model_dim, n_heads, dropout=dropout, qk_norm=True)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_out = nn.LayerNorm(model_dim)\n",
    "\n",
    "        # output projection; tying is common (input/output embeddings)\n",
    "        self.to_logits = nn.Linear(model_dim, vocab_size, bias=False)\n",
    "        self.to_logits.weight = self.tok_emb.weight  # embedding tying\n",
    "\n",
    "        nn.init.normal_(self.pos_spatial, std=0.02)\n",
    "        nn.init.normal_(self.pos_temporal, std=0.02)\n",
    "\n",
    "    def forward_teacher_forcing(self, z_all: torch.Tensor, s: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Teacher forcing forward over all 6 frames.\n",
    "        z_all: (B, 6, 32, 32)  (past + future ground-truth)\n",
    "        s:     (B, 64, 25)\n",
    "        returns logits: (B, 6, 32, 32, vocab)\n",
    "        \"\"\"\n",
    "        B, T, H, W = z_all.shape\n",
    "        assert T == self.T_total and H == self.H and W == self.W\n",
    "\n",
    "        x = self.tok_emb(z_all)  # (B, T, H, W, C)\n",
    "\n",
    "        # add (temporal + spatial) positions\n",
    "        x = x + self.pos_temporal[:, :T] + self.pos_spatial  # broadcast\n",
    "\n",
    "        # add state conditioning as additive embedding per frame\n",
    "        cond = self.state_enc(s, T=T)  # (B, T, C)\n",
    "        x = x + cond[:, :, None, None, :]  # (B,T,1,1,C)\n",
    "\n",
    "        # flatten to (B, T*HW, C)\n",
    "        x = rearrange(x, \"b t h w c -> b (t h w) c\")\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, T=T, HW=self.HW)\n",
    "\n",
    "        x = self.ln_out(x)\n",
    "        x = rearrange(x, \"b (t hw) c -> b t hw c\", t=T, hw=self.HW)\n",
    "        logits = self.to_logits(x)  # (B, T, HW, vocab)\n",
    "        logits = rearrange(logits, \"b t (h w) v -> b t h w v\", h=self.H, w=self.W)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_greedy(self, z_past: torch.Tensor, s: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Greedy autoregressive generation of future frames (3 frames),\n",
    "        generating a full 32x32 grid each step.\n",
    "\n",
    "        z_past: (B, 3, 32, 32)\n",
    "        returns z_hat_future: (B, 3, 32, 32)\n",
    "        \"\"\"\n",
    "        B, Tp, H, W = z_past.shape\n",
    "        assert Tp == self.n_past and H == self.H and W == self.W\n",
    "\n",
    "        z_ctx = z_past.clone()\n",
    "\n",
    "        for t in range(self.n_future):\n",
    "            # Build a 6-frame tensor with placeholders for unknown future\n",
    "            # Here we do stepwise frame generation: at step k, we have past + already generated k frames.\n",
    "            T_current = self.n_past + t + 1\n",
    "            z_all = torch.zeros(B, self.T_total, H, W, device=z_ctx.device, dtype=z_ctx.dtype)\n",
    "            z_all[:, :T_current] = z_ctx  # fill known tokens\n",
    "\n",
    "            # Teacher-forcing forward uses provided z_all; unknown frames are zeros (ignored by masking below)\n",
    "            logits = self.forward_teacher_forcing(z_all, s)  # (B, 6, H, W, V)\n",
    "\n",
    "            # take logits for the next frame index (T_current-1) (the last known frame position)\n",
    "            # Actually we want to predict frame at index T_current-1 (new frame position).\n",
    "            # We used z_ctx including new frame position as zeros, but logits are computed for all frames.\n",
    "            next_idx = T_current - 1\n",
    "            next_logits = logits[:, next_idx]  # (B, H, W, V)\n",
    "            next_tokens = next_logits.argmax(dim=-1)  # (B, H, W)\n",
    "\n",
    "            # append generated frame\n",
    "            z_ctx = torch.cat([z_ctx, next_tokens[:, None]], dim=1)\n",
    "\n",
    "        z_hat_future = z_ctx[:, self.n_past:self.n_past + self.n_future]\n",
    "        return z_hat_future\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35996f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "#  Dataset Template\n",
    "# -------------------------\n",
    "import os, sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from src.dataset import RawTokenDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39d79f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "#  Train / Eval loops\n",
    "# -------------------------\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    vocab_size: int\n",
    "    lr: float = 8e-4\n",
    "    weight_decay: float = 0.05\n",
    "    epochs: int = 10\n",
    "    batch_size: int = 8\n",
    "    num_workers: int = 2\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    log_every: int = 50\n",
    "\n",
    "def compute_loss_ce_future(logits: torch.Tensor, z_all: torch.Tensor, n_past: int = 3) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    logits: (B, 6, 32, 32, V)\n",
    "    z_all:  (B, 6, 32, 32)\n",
    "    Only compute CE on future frames 3..5\n",
    "    \"\"\"\n",
    "    B, T, H, W, V = logits.shape\n",
    "    assert T == 6\n",
    "    logits_f = logits[:, n_past:]           # (B, 3, H, W, V)\n",
    "    target_f = z_all[:, n_past:]            # (B, 3, H, W)\n",
    "\n",
    "    loss = F.cross_entropy(\n",
    "        logits_f.reshape(-1, V),\n",
    "        target_f.reshape(-1),\n",
    "        reduction=\"mean\",\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_autoreg_ce(model: RevontuliCompressionModel, loader: DataLoader, device: str) -> float:\n",
    "    \"\"\"\n",
    "    Simple autoregressive evaluation:\n",
    "      - generate future frames greedily\n",
    "      - compute CE of generated vs GT? (CE usually requires logits; here we approximate with token accuracy)\n",
    "    For leaderboard-like CE you want logits under AR rollout; that’s more involved.\n",
    "    This function provides a sanity-check metric (token accuracy).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for z_all, s in loader:\n",
    "        z_all = z_all.to(device)\n",
    "        s = s.to(device)\n",
    "        z_past = z_all[:, :3]\n",
    "        z_gt_f = z_all[:, 3:]\n",
    "\n",
    "        z_hat_f = model.generate_greedy(z_past, s)  # (B,3,32,32)\n",
    "        total += z_gt_f.numel()\n",
    "        correct += (z_hat_f == z_gt_f).sum().item()\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "def train_one_run(model, train_loader, val_loader, cfg: TrainConfig):\n",
    "    model.to(cfg.device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay, betas=(0.9, 0.95))\n",
    "    # simple warmup+linear decay scheduler template\n",
    "    total_steps = cfg.epochs * len(train_loader)\n",
    "    warmup_steps = min(2000, max(100, total_steps // 20))\n",
    "\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return float(step) / float(max(1, warmup_steps))\n",
    "        # linear decay\n",
    "        return max(0.0, float(total_steps - step) / float(max(1, total_steps - warmup_steps)))\n",
    "\n",
    "    sched = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
    "\n",
    "    step = 0\n",
    "    for epoch in range(cfg.epochs):\n",
    "        model.train()\n",
    "        for z_all, s in train_loader:\n",
    "            z_all = z_all.to(cfg.device)\n",
    "            s = s.to(cfg.device)\n",
    "\n",
    "            logits = model.forward_teacher_forcing(z_all, s)\n",
    "            loss = compute_loss_ce_future(logits, z_all, n_past=model.n_past)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            sched.step()\n",
    "\n",
    "            if step % cfg.log_every == 0:\n",
    "                print(f\"[step {step:07d}] loss={loss.item():.4f} lr={sched.get_last_lr()[0]:.2e}\")\n",
    "            step += 1\n",
    "\n",
    "        # quick sanity eval\n",
    "        acc = eval_autoreg_ce(model, val_loader, cfg.device)\n",
    "        print(f\"[epoch {epoch:03d}] AR token-acc={acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bca933",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "#  Example usage (dummy)\n",
    "# -------------------------\n",
    "\n",
    "def main():\n",
    "    # Dummy data (replace with your real token/state loading)\n",
    "    N = 64\n",
    "    vocab = 4096  # <-- Cosmos tokenizer vocab sizeに合わせて設定\n",
    "    z = torch.randint(0, vocab, (N, 6, 32, 32), dtype=torch.long)\n",
    "    s = torch.randn(N, 64, 25)\n",
    "\n",
    "    ds = TokenStateDataset({\"z\": z, \"s\": s})\n",
    "    train_ds, val_ds = torch.utils.data.random_split(ds, [int(N*0.8), N - int(N*0.8)])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    cfg = TrainConfig(vocab_size=vocab, epochs=2, batch_size=4)\n",
    "\n",
    "    model = RevontuliCompressionModel(\n",
    "        vocab_size=vocab,\n",
    "        model_dim=512,\n",
    "        n_layers=6,     # まずは小さく（本番は24）\n",
    "        n_heads=8,\n",
    "        dropout=0.1,\n",
    "        state_dim=25,\n",
    "        H=32, W=32,\n",
    "        n_past=3,\n",
    "        n_future=3,\n",
    "    )\n",
    "\n",
    "    train_one_run(model, train_loader, val_loader, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7f1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
